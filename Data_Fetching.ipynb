{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Academic Article Summarization - Data Gathering.\n",
        "\n",
        "The data was collected from The [National Center for Biotechnology Information](https://www.ncbi.nlm.nih.gov/) - (NCBI) and under their compliances and policy. NCBI holds an open access database for academic articles, all on topics of science, health and medicine. In total were gather by me 87 different articles, but in order to make my work more focused, i've collected aricles that contained any of the keywords relevant to my field, which are: NLP, Machine Learning, Deep Learning, AI and Neural Netword.\n",
        "\n",
        "Below presented the code that responsiable for fetching the articles and saving them localy on my machine. In order not to overly used the generousity of the NCBI's API for data gathering, and to comply with their request for its use, ive exported the script away from my main NLP model research, in order to overload their server with requests.\n",
        "\n",
        "Thia means, that ive gather the articles from NCBI, but al farther access and text manipulation were done without API calls to their server.\n",
        "\n",
        "And at this point i want to thank to NCBI for allowing both reaserchers and developers to access the data in for our perpouses."
      ],
      "metadata": {
        "id": "3jgXnTTqVfUE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Etz84kontNM"
      },
      "outputs": [],
      "source": [
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mount = '/content/drive'\n",
        "from google.colab import drive\n",
        "drive.mount(mount)"
      ],
      "metadata": {
        "id": "a1ffTOAAn0pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/NLP/PMC_Centralized_data\""
      ],
      "metadata": {
        "id": "2XYqkALQn280"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import re\n",
        "from Bio import Entrez\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Configure email for NCBI access\n",
        "# Feel free to contact me!\n",
        "Entrez.email = \"ramdvlp@gmail.com\"\n",
        "\n",
        "def sanitize_filename(text):\n",
        "    \"\"\" Clean and truncate the text to create a valid filename. \"\"\"\n",
        "    text = re.sub(r'[\\\\/:\"*?<>|]+', \"\", text)\n",
        "    return text[:50]  # Truncate filename if too long\n",
        "\n",
        "def parse_article(xml_content):\n",
        "    \"\"\" Parse XML content and extract title, abstract, and body text for each article. \"\"\"\n",
        "    root = ET.fromstring(xml_content)\n",
        "    articles = []\n",
        "\n",
        "    for article in root.findall('.//article'):\n",
        "        title_elem = article.find('.//article-title')\n",
        "        if title_elem is None or title_elem.text is None:\n",
        "            continue  # Skip article if title is missing\n",
        "\n",
        "        abstract_elem = article.find('.//abstract')\n",
        "        if abstract_elem is None:\n",
        "            continue  # Skip article if abstract is missing\n",
        "        abstract = \"\\n\".join((p.text if p.text else '') for p in abstract_elem.findall('.//p'))\n",
        "        if not abstract:\n",
        "            continue  # Skip article if abstract text is missing\n",
        "\n",
        "        body_elems = article.findall('.//body//p')\n",
        "        body_text = \"\\n\".join((p.text if p.text else '') for p in body_elems)\n",
        "        if not body_text:\n",
        "            continue  # Skip article if body text is missing\n",
        "\n",
        "        articles.append((title_elem.text, abstract, body_text))\n",
        "\n",
        "    return articles\n",
        "\n",
        "def fetch_and_save_articles(keywords, folder, max_count=100):\n",
        "    \"\"\" Search articles containing any of the specified keywords in their title or abstract, and save them without duplicates. \"\"\"\n",
        "    query = f\"({' OR '.join([f'{kw}[Title/Abstract]' for kw in keywords])}) AND open access[filter]\"\n",
        "    handle = Entrez.esearch(db=\"pmc\", term=query, retmax=max_count)\n",
        "    record = Entrez.read(handle)\n",
        "    handle.close()\n",
        "\n",
        "    id_list = record[\"IdList\"]\n",
        "    fetched_ids = set()  # To track already fetched articles\n",
        "    if id_list:\n",
        "        if not os.path.exists(folder):\n",
        "            os.makedirs(folder)\n",
        "\n",
        "        for article_id in id_list:\n",
        "            if article_id in fetched_ids:\n",
        "                continue  # Skip if already fetched\n",
        "            fetched_ids.add(article_id)\n",
        "\n",
        "            handle = Entrez.efetch(db=\"pmc\", id=article_id, rettype=\"medline\", retmode=\"xml\")\n",
        "            article_xml = handle.read()\n",
        "            handle.close()\n",
        "\n",
        "            articles = parse_article(article_xml.decode('utf-8'))\n",
        "            for title, abstract, body_text in articles:\n",
        "                filename = sanitize_filename(title) + \".txt\"\n",
        "                filepath = os.path.join(folder, filename)\n",
        "                with open(filepath, \"w\") as file:\n",
        "                    file.write(\"Title: \" + title + \"\\n\\nAbstract:\\n\" + abstract + \"\\n\\nBody:\\n\" + body_text)\n",
        "\n",
        "            time.sleep(1)  # Sleep to ensure compliance with API rate limits\n",
        "\n",
        "# Keywords and folder configuration\n",
        "keywords = [\"machine learning\", \"deep learning\", \"NLP\", \"artificial intelligence\", \"neural networks\"]\n",
        "save_folder = \"/content/drive/MyDrive/Colab Notebooks/NLP/PMC_Centralized_data\"\n",
        "\n",
        "# Fetch and save articles\n",
        "print(\"Fetching and saving articles...\")\n",
        "fetch_and_save_articles(keywords, save_folder)\n",
        "print(\"All articles fetched and saved successfully.\")"
      ],
      "metadata": {
        "id": "HuAzwva9n5Lx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}